{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext jupyter_black\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GPT-3](https://platform.openai.com/examples/default-adv-tweet-classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to compare two different approaches to text classification with GPT-3\n",
    "# 1. Zero-shot classification\n",
    "# 2. Few-shot classification\n",
    "# Few-shot will no doubt be more accurate, but it is more costly due to the extra prompt tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data that we will use for comparison\n",
    "from src.data.make_dataset import load_dataset_from_file\n",
    "from src.config import DATASET_PATH\n",
    "\n",
    "dataset = load_dataset_from_file(DATASET_PATH)\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipelines\n",
    "# NOTE: The pipelines are async callables, so we need to use `await` to get the results\n",
    "from src.pipelines.openai import get_openai_pipelines\n",
    "\n",
    "pipelines = get_openai_pipelines()\n",
    "zero_shot_pipeline = pipelines[\"zero_shot\"]\n",
    "few_shot_pipeline = pipelines[\"few_shot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try them out on an example from the test set before we compare them\n",
    "example = test_data[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the zero-shot pipeline\n",
    "await zero_shot_pipeline([example[\"entry\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model correctly classifies the example as \"NEUTRAL\" which is a good sign\n",
    "# What about our few-shot pipeline?\n",
    "# NOTE: The few-shot pipeline takes a list of (title, text) tuples as input\n",
    "await few_shot_pipeline([(example[\"title\"], example[\"entry\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also correct! Let's just let both pipelines run on the entire test set and see how they do"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero vs. Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the zero-shot pipeline on the test set\n",
    "zero_shot_labels = await zero_shot_pipeline([entry[\"entry\"] for entry in test_data])\n",
    "\n",
    "# Run the few-shot pipeline on the test set\n",
    "few_shot_labels = await few_shot_pipeline(\n",
    "    [(entry[\"title\"], entry[\"entry\"]) for entry in test_data]\n",
    ")\n",
    "\n",
    "assert len(zero_shot_labels) == len(few_shot_labels) == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy of each pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Zero-shot\n",
    "print(\"Zero-shot classification report:\")\n",
    "print(classification_report(test_data[\"sentiment_output\"], zero_shot_labels))\n",
    "\n",
    "# Few-shot\n",
    "print(\"Few-shot classification report:\")\n",
    "print(classification_report(test_data[\"sentiment_output\"], few_shot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both pipelines are performing well compared to the baseline (which is ~0.69 accuracy on the entire dataset and ~0.72 on the test set)\n",
    "# It's a good thing we checked our assumptions about zero-shot vs. few-shot\n",
    "# It looks like the few-shot pipeline is significantly less accurate when it comes to classifying \"NEGATIVE\" examples\n",
    "# Obviously, trying out different examples in the prompt (i.e. \"prompt engineering\") would be a good next step to\n",
    "# improve the performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
