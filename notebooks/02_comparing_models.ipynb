{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext jupyter_black\n",
    "%matplotlib inline\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare the models on our sentiment analysis task, we need three things:\n",
    "# 1. A test dataset\n",
    "# 2. A set of models to compare (as pipelines, so we can call them on a list of texts and get a list of labels back)\n",
    "# 3. A function that takes the models and test data and returns accuracy metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since our dataset is pretty small at 300 examples, we'll set aside 20% of the data for testing\n",
    "# We'll use the same split for all of our models\n",
    "# We'll use the dataset from the previous notebook\n",
    "from src.config import DATASET_PATH\n",
    "from src.data.make_dataset import load_dataset_from_file\n",
    "\n",
    "dataset = load_dataset_from_file(DATASET_PATH)\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "assert len(test_dataset) == 60  # 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compute the baseline accuracy of the test set\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "baseline_report = classification_report(\n",
    "    test_dataset[\"sentiment_output\"], test_dataset[\"annotated_sentiment\"]\n",
    ")\n",
    "print(baseline_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 72% accuracy is the number to beat\n",
    "# let's store all reports in the same list\n",
    "title_reports = [(\"baseline\", baseline_report)]\n",
    "\n",
    "# and use this function to print them all at the end\n",
    "def print_reports(reports: list[tuple[str, str]]) -> None:\n",
    "    \"\"\"Print a list of reports\n",
    "\n",
    "    Args:\n",
    "        reports (list[tuple[str, str]]): list of (title, report) tuples\n",
    "    \"\"\"\n",
    "    for title, report in reports:\n",
    "        print(f\"{title.upper()}:\")\n",
    "        print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential transfer learning is state-of-the-art for sentiment analysis.\n",
    "# So we'll dive into using fine-tuned transformers from the HuggingFace model hub.\n",
    "# We will also test [SetFit](https://arxiv.org/abs/2209.11055), a new few-shot fine-tuning method.\n",
    "# And [GPT-3](https://platform.openai.com/examples/default-adv-tweet-classifier), because it's so hot right now.\n",
    "\n",
    "from src.pipelines import get_all_pipelines\n",
    "\n",
    "pipes = get_all_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the HuggingFace transformer pipelines first\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "hf_reports = []\n",
    "for name, pipe in pipes[\"hf\"].items():\n",
    "    print(name)\n",
    "    preds = pipe(test_dataset[\"entry\"])  # list of dicts (keys: \"label\", \"score\")\n",
    "    preds = [pred[\"label\"] for pred in preds]  # convert to list of labels\n",
    "    report = classification_report(test_dataset[\"sentiment_output\"], preds)\n",
    "    hf_reports.append((name, report))\n",
    "    print(report)\n",
    "\n",
    "title_reports.extend(hf_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the fine-tuned RoBERTa model from Hartmann et al. (2021) is the best performer\n",
    "# at 75% accuracy, beating the baseline of 72% accuracy on the test set.\n",
    "# It's a RoBERTa-based model fine-tuned on 5,304 manually annotated social media posts\n",
    "# See https://journals.sagepub.com/doi/full/10.1177/00222437211037258 for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the SetFit pipeline\n",
    "preds = pipes[\"setfit\"][\"setfit\"](test_dataset[\"entry\"])\n",
    "setfit_report = classification_report(test_dataset[\"sentiment_output\"], preds)\n",
    "title_reports.append((\"setfit\", setfit_report))\n",
    "print(setfit_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% accuracy is pretty good for a model trained on only 240 examples\n",
    "# This is the best accuracy we've seen so far\n",
    "# It's also VERY fast for both training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally let's try OpenAI's GPT-3 (text-davinci-003)\n",
    "# with both zero-shot and few-shot prompts\n",
    "# NOTE: OpenAI pipes are async\n",
    "print(\"OpenAI: Zero-shot\")\n",
    "preds = await pipes[\"openai\"][\"zero_shot\"](test_dataset[\"entry\"])\n",
    "zero_shot_report = classification_report(test_dataset[\"sentiment_output\"], preds)\n",
    "title_reports.append((\"openai_zero_shot\", zero_shot_report))\n",
    "print(zero_shot_report)\n",
    "\n",
    "print(\"OpenAI: Few-shot\")\n",
    "preds = await pipes[\"openai\"][\"few_shot\"](\n",
    "    zip(test_dataset[\"title\"], test_dataset[\"entry\"])\n",
    ")\n",
    "few_shot_report = classification_report(test_dataset[\"sentiment_output\"], preds)\n",
    "title_reports.append((\"openai_few_shot\", few_shot_report))\n",
    "print(few_shot_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print all the reports\n",
    "print_reports(title_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see from the reports, the SetFit pipeline is the best performer at 80% accuracy"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
