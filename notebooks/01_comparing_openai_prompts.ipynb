{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%load_ext jupyter_black\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [GPT-3](https://platform.openai.com/examples/default-adv-tweet-classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to compare two different approaches to text classification with GPT-3\n",
    "# 1. Zero-shot classification\n",
    "# 2. Few-shot classification\n",
    "# Few-shot will no doubt be more accurate, but it is more costly due to the extra prompt tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data that we will use for comparison\n",
    "from src.data.make_dataset import load_dataset_from_file\n",
    "from src.config import DATASET_PATH\n",
    "\n",
    "dataset = load_dataset_from_file(DATASET_PATH)\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipelines\n",
    "# NOTE: The pipelines are async callables, so we need to use `await` to get the results\n",
    "from src.pipelines.openai import get_openai_pipelines\n",
    "\n",
    "pipelines = get_openai_pipelines()\n",
    "zero_shot_pipeline = pipelines[\"zero_shot\"]\n",
    "few_shot_pipeline = pipelines[\"few_shot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'team_id': 988,\n",
       " 'id': 2309258,\n",
       " 'title': 'taking break last 15 min before shift is over',\n",
       " 'entry': 'iâ€™ve heard mixed opinions from my fellow coworkers on this topic. are you allowed to take your last 15 minute break in the last 15 min or so of when your break is over?',\n",
       " 'data_source': 'Reddit',\n",
       " 'sentiment_output': 'NEUTRAL',\n",
       " 'annotated_sentiment': 'NEUTRAL',\n",
       " 'correct?': 1,\n",
       " 'label_str': 'NEUTRAL',\n",
       " 'label': 1,\n",
       " '__index_level_0__': 203}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try them out on an example from the test set before we compare them\n",
    "example = test_data[0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEUTRAL']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the zero-shot pipeline\n",
    "await zero_shot_pipeline([example[\"entry\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEUTRAL']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The model correctly classifies the example as \"NEUTRAL\" which is a good sign\n",
    "# What about our few-shot pipeline?\n",
    "# NOTE: The few-shot pipeline takes a list of (title, text) tuples as input\n",
    "await few_shot_pipeline([(example[\"title\"], example[\"entry\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also correct! Let's just let both pipelines run on the entire test set and see how they do"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero vs. Few-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the zero-shot pipeline on the test set\n",
    "zero_shot_labels = await zero_shot_pipeline([entry[\"entry\"] for entry in test_data])\n",
    "\n",
    "# Run the few-shot pipeline on the test set\n",
    "few_shot_labels = await few_shot_pipeline(\n",
    "    [(entry[\"title\"], entry[\"entry\"]) for entry in test_data]\n",
    ")\n",
    "\n",
    "assert len(zero_shot_labels) == len(few_shot_labels) == len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.88      0.70      0.78        30\n",
      "     NEUTRAL       0.53      0.56      0.55        16\n",
      "    POSITIVE       0.74      1.00      0.85        14\n",
      "\n",
      "    accuracy                           0.73        60\n",
      "   macro avg       0.71      0.75      0.72        60\n",
      "weighted avg       0.75      0.73      0.73        60\n",
      "\n",
      "Few-shot classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.83      0.63      0.72        30\n",
      "     NEUTRAL       0.52      0.69      0.59        16\n",
      "    POSITIVE       0.88      1.00      0.93        14\n",
      "\n",
      "    accuracy                           0.73        60\n",
      "   macro avg       0.74      0.77      0.75        60\n",
      "weighted avg       0.76      0.73      0.73        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of each pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Zero-shot\n",
    "print(\"Zero-shot classification report:\")\n",
    "print(classification_report(test_data[\"sentiment_output\"], zero_shot_labels))\n",
    "\n",
    "# Few-shot\n",
    "print(\"Few-shot classification report:\")\n",
    "print(classification_report(test_data[\"sentiment_output\"], few_shot_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both pipelines are performing well compared to the baseline (which is ~0.69 accuracy on the entire dataset and ~0.72 on the test set)\n",
    "# It's a good thing we checked our assumptions about zero-shot vs. few-shot\n",
    "# It looks like the few-shot pipeline is significantly less accurate when it comes to classifying \"NEGATIVE\" examples\n",
    "# Obviously, trying out different examples in the prompt (i.e. \"prompt engineering\") would be a good next step to\n",
    "# improve the performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46ff26dcb9cd07a10dafeb937c68bae38a96d110c93284887f98c642a2901c72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
