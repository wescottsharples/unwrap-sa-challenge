{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext jupyter_black\n",
    "%matplotlib inline\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare the models on our sentiment analysis task, we need three things:\n",
    "# 1. A test dataset\n",
    "# 2. A set of models to compare (as pipelines, so we can call them on a list of texts and get a list of labels back)\n",
    "# 3. A function that takes the models and test data and returns accuracy metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since our dataset is pretty small at 300 examples, we'll set aside 20% of the data for testing\n",
    "# We'll use the same split for all of our models\n",
    "# We'll use the dataset from the previous notebook\n",
    "from src.config import DATASET_PATH\n",
    "from src.data.make_dataset import load_dataset_from_file\n",
    "\n",
    "dataset = load_dataset_from_file(DATASET_PATH)\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "assert len(test_dataset) == 60  # 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.79      0.63      0.70        30\n",
      "     NEUTRAL       0.50      0.69      0.58        16\n",
      "    POSITIVE       0.93      0.93      0.93        14\n",
      "\n",
      "    accuracy                           0.72        60\n",
      "   macro avg       0.74      0.75      0.74        60\n",
      "weighted avg       0.75      0.72      0.72        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's compute the baseline accuracy of the test set\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "baseline_report = classification_report(\n",
    "    test_dataset[\"sentiment_output\"], test_dataset[\"annotated_sentiment\"]\n",
    ")\n",
    "print(baseline_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 72% accuracy is the number to beat\n",
    "# let's store all reports in the same list\n",
    "title_reports = [(\"baseline\", baseline_report)]\n",
    "\n",
    "# and use this function to print them all at the end\n",
    "def print_reports(reports: list[tuple[str, str]]) -> None:\n",
    "    \"\"\"Print a list of reports\n",
    "\n",
    "    Args:\n",
    "        reports (list[tuple[str, str]]): list of (title, report) tuples\n",
    "    \"\"\"\n",
    "    for title, report in reports:\n",
    "        print(f\"{title.upper()}:\")\n",
    "        print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential transfer learning is state-of-the-art for sentiment analysis.\n",
    "# So we'll dive into using fine-tuned transformers from the HuggingFace model hub.\n",
    "# We will also test [SetFit](https://arxiv.org/abs/2209.11055), a new few-shot fine-tuning method.\n",
    "# And [GPT-3](https://platform.openai.com/examples/default-adv-tweet-classifier), because it's so hot right now.\n",
    "\n",
    "from src.pipelines import get_all_pipelines\n",
    "\n",
    "pipes = get_all_pipelines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta_cardiffnlp\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.77      0.67      0.71        30\n",
      "     NEUTRAL       0.50      0.50      0.50        16\n",
      "    POSITIVE       0.72      0.93      0.81        14\n",
      "\n",
      "    accuracy                           0.68        60\n",
      "   macro avg       0.66      0.70      0.68        60\n",
      "weighted avg       0.69      0.68      0.68        60\n",
      "\n",
      "bert_seethal\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.84      0.53      0.65        30\n",
      "     NEUTRAL       0.52      0.75      0.62        16\n",
      "    POSITIVE       0.72      0.93      0.81        14\n",
      "\n",
      "    accuracy                           0.68        60\n",
      "   macro avg       0.70      0.74      0.69        60\n",
      "weighted avg       0.73      0.68      0.68        60\n",
      "\n",
      "roberta_hartmann\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.74      0.77      0.75        30\n",
      "     NEUTRAL       0.60      0.56      0.58        16\n",
      "    POSITIVE       0.93      0.93      0.93        14\n",
      "\n",
      "    accuracy                           0.75        60\n",
      "   macro avg       0.76      0.75      0.75        60\n",
      "weighted avg       0.75      0.75      0.75        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's compare the HuggingFace transformer pipelines first\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "hf_reports = []\n",
    "for name, pipe in pipes[\"hf\"].items():\n",
    "    print(name)\n",
    "    preds = pipe(test_dataset[\"entry\"])  # list of dicts (keys: \"label\", \"score\")\n",
    "    preds = [pred[\"label\"] for pred in preds]  # convert to list of labels\n",
    "    report = classification_report(test_dataset[\"sentiment_output\"], preds)\n",
    "    hf_reports.append((name, report))\n",
    "    print(report)\n",
    "\n",
    "title_reports.extend(hf_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the fine-tuned RoBERTa model from Hartmann et al. (2021) is the best performer\n",
    "# at 75% accuracy, beating the baseline of 72% accuracy on the test set.\n",
    "# It's a RoBERTa-based model fine-tuned on 5,304 manually annotated social media posts\n",
    "# See https://journals.sagepub.com/doi/full/10.1177/00222437211037258 for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.83      0.83      0.83        30\n",
      "     NEUTRAL       0.65      0.69      0.67        16\n",
      "    POSITIVE       0.92      0.86      0.89        14\n",
      "\n",
      "    accuracy                           0.80        60\n",
      "   macro avg       0.80      0.79      0.80        60\n",
      "weighted avg       0.80      0.80      0.80        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now the SetFit pipeline\n",
    "preds = pipes[\"setfit\"][\"setfit\"](test_dataset[\"entry\"])\n",
    "setfit_report = classification_report(test_dataset[\"sentiment_output\"], preds)\n",
    "title_reports.append((\"setfit\", setfit_report))\n",
    "print(setfit_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% accuracy is pretty good for a model trained on only 240 examples\n",
    "# This is the best accuracy we've seen so far\n",
    "# It's also VERY fast for both training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI: Zero-shot\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.88      0.70      0.78        30\n",
      "     NEUTRAL       0.53      0.56      0.55        16\n",
      "    POSITIVE       0.74      1.00      0.85        14\n",
      "\n",
      "    accuracy                           0.73        60\n",
      "   macro avg       0.71      0.75      0.72        60\n",
      "weighted avg       0.75      0.73      0.73        60\n",
      "\n",
      "OpenAI: Few-shot\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.83      0.63      0.72        30\n",
      "     NEUTRAL       0.52      0.69      0.59        16\n",
      "    POSITIVE       0.88      1.00      0.93        14\n",
      "\n",
      "    accuracy                           0.73        60\n",
      "   macro avg       0.74      0.77      0.75        60\n",
      "weighted avg       0.76      0.73      0.73        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Finally let's try OpenAI's GPT-3 (text-davinci-003)\n",
    "# with both zero-shot and few-shot prompts\n",
    "# NOTE: OpenAI pipes are async\n",
    "print(\"OpenAI: Zero-shot\")\n",
    "preds = await pipes[\"openai\"][\"zero_shot\"](test_dataset[\"entry\"])\n",
    "zero_shot_report = classification_report(test_dataset[\"sentiment_output\"], preds)\n",
    "title_reports.append((\"openai_zero_shot\", zero_shot_report))\n",
    "print(zero_shot_report)\n",
    "\n",
    "print(\"OpenAI: Few-shot\")\n",
    "preds = await pipes[\"openai\"][\"few_shot\"](\n",
    "    zip(test_dataset[\"title\"], test_dataset[\"entry\"])\n",
    ")\n",
    "few_shot_report = classification_report(test_dataset[\"sentiment_output\"], preds)\n",
    "title_reports.append((\"openai_few_shot\", few_shot_report))\n",
    "print(few_shot_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASELINE:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.79      0.63      0.70        30\n",
      "     NEUTRAL       0.50      0.69      0.58        16\n",
      "    POSITIVE       0.93      0.93      0.93        14\n",
      "\n",
      "    accuracy                           0.72        60\n",
      "   macro avg       0.74      0.75      0.74        60\n",
      "weighted avg       0.75      0.72      0.72        60\n",
      "\n",
      "ROBERTA_CARDIFFNLP:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.77      0.67      0.71        30\n",
      "     NEUTRAL       0.50      0.50      0.50        16\n",
      "    POSITIVE       0.72      0.93      0.81        14\n",
      "\n",
      "    accuracy                           0.68        60\n",
      "   macro avg       0.66      0.70      0.68        60\n",
      "weighted avg       0.69      0.68      0.68        60\n",
      "\n",
      "BERT_SEETHAL:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.84      0.53      0.65        30\n",
      "     NEUTRAL       0.52      0.75      0.62        16\n",
      "    POSITIVE       0.72      0.93      0.81        14\n",
      "\n",
      "    accuracy                           0.68        60\n",
      "   macro avg       0.70      0.74      0.69        60\n",
      "weighted avg       0.73      0.68      0.68        60\n",
      "\n",
      "ROBERTA_HARTMANN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.74      0.77      0.75        30\n",
      "     NEUTRAL       0.60      0.56      0.58        16\n",
      "    POSITIVE       0.93      0.93      0.93        14\n",
      "\n",
      "    accuracy                           0.75        60\n",
      "   macro avg       0.76      0.75      0.75        60\n",
      "weighted avg       0.75      0.75      0.75        60\n",
      "\n",
      "SETFIT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.83      0.83      0.83        30\n",
      "     NEUTRAL       0.65      0.69      0.67        16\n",
      "    POSITIVE       0.92      0.86      0.89        14\n",
      "\n",
      "    accuracy                           0.80        60\n",
      "   macro avg       0.80      0.79      0.80        60\n",
      "weighted avg       0.80      0.80      0.80        60\n",
      "\n",
      "OPENAI_ZERO_SHOT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.88      0.70      0.78        30\n",
      "     NEUTRAL       0.53      0.56      0.55        16\n",
      "    POSITIVE       0.74      1.00      0.85        14\n",
      "\n",
      "    accuracy                           0.73        60\n",
      "   macro avg       0.71      0.75      0.72        60\n",
      "weighted avg       0.75      0.73      0.73        60\n",
      "\n",
      "OPENAI_FEW_SHOT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    NEGATIVE       0.83      0.63      0.72        30\n",
      "     NEUTRAL       0.52      0.69      0.59        16\n",
      "    POSITIVE       0.88      1.00      0.93        14\n",
      "\n",
      "    accuracy                           0.73        60\n",
      "   macro avg       0.74      0.77      0.75        60\n",
      "weighted avg       0.76      0.73      0.73        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's print all the reports\n",
    "print_reports(title_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see from the reports, the SetFit pipeline is the best performer at 80% accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46ff26dcb9cd07a10dafeb937c68bae38a96d110c93284887f98c642a2901c72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
